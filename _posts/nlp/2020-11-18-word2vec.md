---
layout: post
title: "Word2Vec"
subtitle: "How to represent words for AI"
categories: ai
tags: nlp
comments: true
---
## How to represent words?
* Discrete representation: The vast majority of rule-based and statistical NLP work
regards words as atomic symbols: hotel, conference, etc.
  * WordNet? - subjective, requires human labor to create and adapt
  * Simple idea: "One-hot encoded vectors" - very sparse vectors
  * Build a vector for a large vocabulary of words
  * Turn on the corresponding element for the word in the vector
  * The problem is that there is no similarity among related words:
   e.g., Seattle motel vs. Seattle hotel
  * localist representation

* Distributional similarity based representations
  * You can get a lot of value by representing a word by means of its neighbors.
  * One of the most successful ideas of modern statistical NLP.

    ```markdown
    You shall know a word by the company it keeps.
      --J. R. Firth (1957)
    ```
    Similar to later Wittgenstein

* Word meaning is defined in terms of vectors:
  * We will build a dense vector for each word type, chosen so that
    it is good at predicting other words appearing in its context...
    those other words also being represented by vectors...it all gets a bit recursive
    
distributed representation != distributional similarity

#### Basic idea of learning neural network word embeddings
We define a model that aims to predict between a center word w<sub>t</sub>
and context words in terms of word vectors.

> p(context|w<sub>t</sub>)

which has a loss function, e.g.,
> J = 1 - p(w<sub>-t</sub>|w<sub>t</sub>)
* -t means all other context words
* We look at many positions t in a big language corpus.
* We keep adjusting the vector representations of words to minimize this loss.

#### Directly learning low-dimensional word vectors
* Learning representations by back-propagating errors (Rumelhart et al. 1986)
* A neural probabilistic language model (Bengio et al., 2003)
* NLP (almost) from Scratch (Collobert & Weston, 2008)
* A recent, even simpler and faster model: word2vec (Mikolov et al. 2013) --> intro now

### Main idea of word2vec
predict between every word and its context

Two algorithms
1. Skip grams: Predict context words given target (position independent)
2. Continuous Bag of Words (CBOW): Predict target word from bag-of-words context

Two (moderately efficient) training methods
1. Hierarchical softmax
2. Negative sampling

* For each word t = 1 ... T, predict surrounding words in a window of
"radius" m of every word.

Objective function: Maximize the probability of context word
given the current center word:

J'(&theta;) = &pi;_{T}^{t =1}


## Reference
* Stanford [NLP with Deep Learning](https://www.youtube.com/watch?v=ERibwqs9p38)